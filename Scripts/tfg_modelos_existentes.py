# -*- coding: utf-8 -*-
"""tfg_modelos_existentes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16vq0glIbSyZ39_w20mAVGTT9k2nSvJIa

###Imports###
"""

import datetime
import numpy as np
import os
import tensorflow as tf
import matplotlib.pyplot as plt
from tqdm import tqdm

from tensorflow.keras import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, concatenate

import sys

#Carga de datos
from pandas import read_csv #Leer el CSV
import tarfile
from PIL import Image, ImageOps  #Librería de imágenes
import io
import pandas as pd

#Mostrar imágenes
import matplotlib.pyplot as plt

#Para cambiar píxeles imagen
import numpy as np

#Casteo a tensor
from tensorflow.keras.preprocessing.image import img_to_array

#Tensorflow
import tensorflow as tf
from tensorflow.keras import layers, models, applications

#Capas modelo
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import Activation, Dense, Flatten, Conv2D, MaxPooling2D, concatenate, GlobalAveragePooling2D, Lambda, Input, BatchNormalization, SeparableConv2D

#Separación de datos
from sklearn import model_selection

#Obtener resultados
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error

#Early stopping
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

import tensorflow.keras.applications as applications


#Modelos EfficientNetV2B3 y ResNet50
#from tensorflow.keras.applications import EfficientNetV2B3
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications import Xception

nuevo_ancho = int(sys.argv[1])
nuevo_alto = nuevo_ancho
canales = int(sys.argv[2])
numero_modelo = int(sys.argv[3])
epocas = int(sys.argv[4])
optimizador = "adam"
perdida = "mean_squared_error"

print(f"Ancho: {nuevo_ancho}")
print(f"Alto: {nuevo_alto}")
print(f"Canales: {canales}")
print(f"Modelo: {numero_modelo}")
print(f"Epocas: {epocas}")

"""###Carga de datos###"""

'''
Módulo de carga de datos
'''

"""###Separar datos###"""

def separar_datos(dataset_entrenar, tamano_validacion, tamano_test, semilla):
  caracteristicas1 = np.stack(dataset_entrenar['primera_imagen'].to_numpy())
  caracteristicas2 = np.stack(dataset_entrenar['segunda_imagen'].to_numpy())
  objetivo = dataset_entrenar['puntuacion'].astype('float32')

  #caracteristicas1_train, caracteristicas1_val, caracteristicas2_train, caracteristicas2_val, etiquetas_train, etiquetas_val = model_selection.train_test_split(
  #    caracteristicas1, caracteristicas2, objetivo, test_size=tamano_test, random_state=semilla)

  caracteristicas1_train, caracteristicas1_temp, caracteristicas2_train, caracteristicas2_temp, etiquetas_train, etiquetas_temp = model_selection.train_test_split(
      caracteristicas1, caracteristicas2, objetivo, test_size=tamano_validacion, random_state=semilla)

  caracteristicas1_val, caracteristicas1_test, caracteristicas2_val, caracteristicas2_test, etiquetas_val, etiquetas_test = model_selection.train_test_split(
      caracteristicas1_temp, caracteristicas2_temp, etiquetas_temp, test_size=tamano_test, random_state=semilla)

  return caracteristicas1_train, caracteristicas1_val, caracteristicas1_test, caracteristicas2_train, caracteristicas2_val, caracteristicas2_test, etiquetas_train, etiquetas_val, etiquetas_test

"""###Modelo AlexNet###"""

def generar_alexnet():
  modelo1 = Sequential([
        Conv2D(filters=96, kernel_size=(11, 11), strides=4, padding='valid', input_shape=(nuevo_ancho, nuevo_alto, canales), activation='relu', kernel_initializer='he_normal'),
        MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same', data_format=None),
        Conv2D(filters=256, kernel_size=(5, 5), strides=1, padding='same', activation='relu', kernel_initializer='he_normal'),
        MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same', data_format=None),
        Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal'),
        Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal'),
        Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal'),
        MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same', data_format=None),
        Flatten()
    ])

  modelo2 = Sequential([
        Conv2D(filters=96, kernel_size=(11, 11), strides=4, padding='valid', input_shape=(nuevo_ancho, nuevo_alto, canales), activation='relu', kernel_initializer='he_normal'),
        MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same', data_format=None),
        Conv2D(filters=256, kernel_size=(5, 5), strides=1, padding='same', activation='relu', kernel_initializer='he_normal'),
        MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same', data_format=None),
        Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal'),
        Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal'),
        Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal'),
        MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same', data_format=None),
        Flatten()
    ])

  concatenacion = concatenate([modelo1.output, modelo2.output])
  capa_densa = Dense(4096, activation='relu')(concatenacion)
  capa_densa = Dense(4096, activation='relu')(concatenacion)
  capa_densa = Dense(1000, activation='relu')(concatenacion)
  capa_output = Dense(1, activation='linear')(capa_densa)

  modeloFinal = Model(inputs=[modelo1.input, modelo2.input], outputs=capa_output)
  modeloFinal.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='mean_squared_error')
  print("AlexNet")
  return modeloFinal

"""##VGG16##"""

def generar_vgg16():
  modelo1 = Sequential([
      Conv2D(filters=64, kernel_size=(3,3), input_shape=(nuevo_ancho, nuevo_alto, canales), padding="same", activation="relu"),
      Conv2D(filters=64, kernel_size=(3,3), padding="same", activation="relu"),
      MaxPooling2D((2, 2)),
      Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"),
      Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"),
      MaxPooling2D((2, 2)),
      Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"),
      Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"),
      Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"),
      MaxPooling2D((2, 2)),
      Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"),
      Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"),
      Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"),
      MaxPooling2D((2, 2)),
      Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"),
      Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"),
      Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"),
      MaxPooling2D((2, 2)),
      Flatten()
  ])

  modelo2 = Sequential([
      Conv2D(filters=64, kernel_size=(3,3), input_shape=(nuevo_ancho, nuevo_alto, canales), padding="same", activation="relu"),
      Conv2D(filters=64, kernel_size=(3,3), padding="same", activation="relu"),
      MaxPooling2D((2, 2)),
      Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"),
      Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"),
      MaxPooling2D((2, 2)),
      Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"),
      Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"),
      Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"),
      MaxPooling2D((2, 2)),
      Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"),
      Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"),
      Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"),
      MaxPooling2D((2, 2)),
      Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"),
      Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"),
      Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"),
      MaxPooling2D((2, 2)),
      Flatten()
  ])

  concatenacion = concatenate([modelo1.output, modelo2.output])

  dense1 = Dense(4096, activation="relu")(concatenacion)
  dense2 = Dense(4096, activation="relu")(dense1)
  output = Dense(1, activation="linear")(dense2)

  modeloVGG16 = Model(inputs=[modelo1.input, modelo2.input], outputs=output)
  modeloVGG16.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='mean_squared_error')
  print("VGG16")
  return modeloVGG16

"""##EfficientNetV2B3##"""

def generar_efficientnet():
  modelo1 = Sequential()
  modelo2 = Sequential()

  efficientNet1 = EfficientNetV2B3(weights='imagenet', include_top=False, input_shape=(nuevo_ancho, nuevo_alto, canales))
  modelo1.add(efficientNet1)
  modelo1.add(GlobalAveragePooling2D())

  efficientNet2 = EfficientNetV2B3(weights='imagenet', include_top=False, input_shape=(nuevo_ancho, nuevo_alto, canales))
  modelo2.add(efficientNet1)
  modelo2.add(GlobalAveragePooling2D())

  concatenacion = concatenate([modelo1.output, modelo2.output])
  capa_output = Dense(1, activation='linear')(concatenacion)

  modeloFinal = Model(inputs=[modelo1.input, modelo2.input], outputs=capa_output)
  modeloFinal.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='mean_squared_error')
  print("EfficientNetV2B3")
  return modeloFinal

"""##XCeption##"""

def entry_flow(inputs) :

    x = Conv2D(32, 3, strides = 2, padding='same')(inputs)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv2D(64,3,padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    previous_block_activation = x

    for size in [128, 256, 728] :

        x = Activation('relu')(x)
        x = SeparableConv2D(size, 3, padding='same')(x)
        x = BatchNormalization()(x)

        x = Activation('relu')(x)
        x = SeparableConv2D(size, 3, padding='same')(x)
        x = BatchNormalization()(x)

        x = MaxPooling2D(3, strides=2, padding='same')(x)

        residual = Conv2D(size, 1, strides=2, padding='same')(previous_block_activation)

        x = tf.keras.layers.Add()([x, residual])
        previous_block_activation = x

    return x

def middle_flow(x, num_blocks=8) :

    previous_block_activation = x

    for _ in range(num_blocks) :

        x = Activation('relu')(x)
        x = SeparableConv2D(728, 3, padding='same')(x)
        x = BatchNormalization()(x)

        x = Activation('relu')(x)
        x = SeparableConv2D(728, 3, padding='same')(x)
        x = BatchNormalization()(x)

        x = Activation('relu')(x)
        x = SeparableConv2D(728, 3, padding='same')(x)
        x = BatchNormalization()(x)

        x = tf.keras.layers.Add()([x, previous_block_activation])
        previous_block_activation = x

    return x
def exit_flow(x) :

    previous_block_activation = x

    x = Activation('relu')(x)
    x = SeparableConv2D(728, 3, padding='same')(x)
    x = BatchNormalization()(x)

    x = Activation('relu')(x)
    x = SeparableConv2D(1024, 3, padding='same')(x)
    x = BatchNormalization()(x)

    x = MaxPooling2D(3, strides=2, padding='same')(x)

    residual = Conv2D(1024, 1, strides=2, padding='same')(previous_block_activation)
    x = tf.keras.layers.Add()([x, residual])

    x = Activation('relu')(x)
    x = SeparableConv2D(728, 3, padding='same')(x)
    x = BatchNormalization()(x)

    x = Activation('relu')(x)
    x = SeparableConv2D(1024, 3, padding='same')(x)
    x = BatchNormalization()(x)

    x = GlobalAveragePooling2D()(x)
    #x = Dense(1, activation='linear')(x)

    return x

def crear_modelo_xception():
  primer_input = Input(shape=(nuevo_alto, nuevo_ancho, canales))
  primer_output = exit_flow(middle_flow(entry_flow(primer_input)))

  segundo_input = Input(shape=(nuevo_alto, nuevo_ancho, canales))
  segundo_output = exit_flow(middle_flow(entry_flow(segundo_input)))

  concatenacion = concatenate([primer_output, segundo_output])

  capa_output = Dense(1, activation='linear')(concatenacion)

  modeloFinal = Model(inputs=[primer_input, segundo_input], outputs=capa_output)

  modeloFinal.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='mean_squared_error')
  print("XCeption")

  return modeloFinal

"""##ResNet50##"""

def generar_resnet50():
  modelo1 = Sequential()
  modelo2 = Sequential()

  input_a = Input(shape=(nuevo_alto, nuevo_ancho, canales), name='input_a')
  input_b = Input(shape=(nuevo_alto, nuevo_ancho, canales), name='input_b')

  resNet1 = ResNet50(weights='imagenet', include_top=False)
  resNet1._name = 'a'
  modelo1.add(input_a)
  modelo1.add(resNet1)
  modelo1.add(GlobalAveragePooling2D())

  resNet2 = ResNet50(weights='imagenet', include_top=False)
  resNet2._name = 'b'
  for layer in resNet2.layers:
      layer._name = 'torre2_' + layer.name

  modelo2.add(input_b)
  modelo2.add(resNet2)
  modelo2.add(GlobalAveragePooling2D())

  concatenacion = concatenate([modelo1.output, modelo2.output])
  capa_output = Dense(1, activation='linear')(concatenacion)

  modeloFinal = Model(inputs=[modelo1.input, modelo2.input], outputs=capa_output)
  modeloFinal.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='mean_squared_error')
  print("ResNet50")
  return modeloFinal

def generar_xception_2():
  modelo1 = Sequential()
  modelo2 = Sequential()

  input_a = Input(shape=(nuevo_alto, nuevo_ancho, canales), name='input_a')
  input_b = Input(shape=(nuevo_alto, nuevo_ancho, canales), name='input_b')

  xception1 = Xception(weights='imagenet', include_top=False)
  xception1._name = 'a'
  modelo1.add(input_a)
  modelo1.add(xception1)
  modelo1.add(GlobalAveragePooling2D())

  xception2 = Xception(weights='imagenet', include_top=False)
  xception2._name = 'b'
  for layer in xception2.layers:
      layer._name = 'torre2_' + layer.name

  modelo2.add(input_b)
  modelo2.add(xception2)
  modelo2.add(GlobalAveragePooling2D())

  concatenacion = concatenate([modelo1.output, modelo2.output])
  capa_output = Dense(1, activation='linear')(concatenacion)

  modeloFinal = Model(inputs=[modelo1.input, modelo2.input], outputs=capa_output)
  modeloFinal.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='mean_squared_error')
  print("Xception 2")
  return modeloFinal

csv = 'dataset-v2.csv'
#url_imagenes = '/content/gdrive/MyDrive/TFG/imagenes.tgz'
url_imagenes = 'imagenes.tgz'

opciones = [None] * 7
#Tamano
opciones[0] = True
#Gris
if canales == 1:
  opciones[1] = True
else:
  opciones[1] = False
#Normalizar
opciones[2] = True
#Blanco-negro
opciones[3] = True
#Eliminar canal
opciones[4] = False
#Eliminar filas
opciones[5] = False
#Eliminar 0
opciones[6] = False


tamano_entrenamiento = 0.4
tamano_test = 0.5
semilla = 1

dataset_entrenar = pd.read_pickle('dataset_entrenar64x64.pkl')

caracteristicas1_train, caracteristicas1_val, caracteristicas1_test, caracteristicas2_train, caracteristicas2_val, caracteristicas2_test, etiquetas_train, etiquetas_val, etiquetas_test = separar_datos(
    dataset_entrenar, tamano_entrenamiento, tamano_test, semilla)

datos = {
    'caracteristicas1_train': caracteristicas1_train,
    'caracteristicas1_val': caracteristicas1_val,
    'caracteristicas1_test': caracteristicas1_test,
    'caracteristicas2_train': caracteristicas2_train,
    'caracteristicas2_val': caracteristicas2_val,
    'caracteristicas2_test': caracteristicas2_test,
    'etiquetas_train': etiquetas_train,
    'etiquetas_val': etiquetas_val,
    'etiquetas_test': etiquetas_test
}
print(dataset_entrenar.shape)

"""###Elección modelo###"""

##Modelo##

with tf.device('/GPU'):
  if numero_modelo == 0:
    modeloFinal = generar_alexnet()
  elif numero_modelo == 1:
    modeloFinal = generar_vgg16()
  elif numero_modelo == 2:
    modeloFinal = generar_efficientnet()
  elif numero_modelo == 3:
    modeloFinal = crear_modelo_xception()
  elif numero_modelo == 4:
    modeloFinal = generar_resnet50()
  elif numero_modelo == 5:
    modeloFinal = generar_xception_2()
pass

historial = modeloFinal.fit([caracteristicas1_train, caracteristicas2_train], etiquetas_train , validation_data=(
      [caracteristicas1_val, caracteristicas2_val], etiquetas_val), epochs=epocas, verbose=0, batch_size=10,
             callbacks=[
    EarlyStopping(monitor='val_loss', mode='min', min_delta=0, patience=40, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.5, patience=20, min_delta=0, min_lr=0.00001)]
             )

predicciones = modeloFinal.predict([caracteristicas1_test, caracteristicas2_test])

mse = mean_squared_error(etiquetas_test, predicciones)
print(f'MSE: {mse}')

mae = mean_absolute_error(etiquetas_test, predicciones)
print(f'MAE: {mae}')

suma_errors = 0
epsilon = 0.0001  # Un valor muy pequeño
for true_val, pred_val in zip(etiquetas_test, predicciones):
    true_val = max(true_val, epsilon)
    suma_errors += abs((true_val - pred_val) / true_val)
mape2 = (suma_errors / len(etiquetas_test)) * 100
print(f'MAPE: {mape2}')

rmse = mean_squared_error(etiquetas_test, predicciones, squared=False)
print("RMSE:", rmse)

r2 = r2_score(etiquetas_test, predicciones)
print("R2:", r2)

etiquetas_test_numpy = etiquetas_test.values.flatten()  # Convertir el DataFrame a una matriz numpy 1D
prediccionesFlatten = predicciones.flatten() 
no_zero_indices = np.where(etiquetas_test != 0)
etiquetas_test_filtered = etiquetas_test_numpy[no_zero_indices]
predicciones_filtered = prediccionesFlatten[no_zero_indices]

suma_errors = 0
epsilon = 0.0001  # Un valor muy pequeño
for true_val, pred_val in zip(etiquetas_test_filtered, predicciones_filtered):
    true_val = max(true_val, epsilon)
    suma_errors += abs((true_val - pred_val) / true_val)
mape = (suma_errors / len(etiquetas_test_filtered)) * 100
print(f'MAPE Filtrado: {mape}')

train_losses = historial.history['loss']
val_losses = historial.history['val_loss']

plt.figure(figsize=(10, 8)) 
plt.plot(train_losses, label='Entrenamiento')
plt.plot(val_losses, label='Validación')
plt.xlabel('Epocas')
plt.ylabel('Perdida (Loss)')
plt.title('Curva de aprendizaje')
plt.legend()

plt.savefig('curva_aprendizajeAlexnet.png')

plt.figure(figsize=(10, 8))
plt.scatter(datos['etiquetas_test'], predicciones, color='blue', alpha=0.6)

# Añadir etiquetas y título
plt.title('Gráfico de Dispersión')
plt.xlabel('Reales')
plt.ylabel('Predicciones')

# Agregar una línea diagonal de referencia (idealmente los puntos deberían estar cerca de esta línea)
plt.plot(datos['etiquetas_test'], datos['etiquetas_test'], color='red', linestyle='--')
plt.savefig('dispersionAlexnet.png')
modeloFinal.save('modeloAlexnet.h5')
