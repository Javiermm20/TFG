# -*- coding: utf-8 -*-
"""tfg_servidor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12shvx8y4SrPpmtQmHcPqMZrsQOewZlKv

***DESARROLLO DE UNA INTELIGENCIA ARTIFICIAL QUE ESTABLEZCA EL GRADO DE SIMILITUD ENTRE MOLÉCULAS***


Autor:

*   Francisco Javier Martínez Moreno



Director:

*   Dr. Antonio Jesús Banegas Luna

##Módulo de importaciones##
"""

'''
Módulo de importaciones

- Este módulo se va ampliando conforme avanza el desarrollo
'''
#Importación de los datasets
import os
import sys
#from google.colab import drive

#Carga de datos
from pandas import read_csv #Leer el CSV
import tarfile
from PIL import Image, ImageOps  #Librería de imágenes
import io
import pandas as pd

#Mostrar imágenes
import matplotlib.pyplot as plt

#Para cambiar píxeles imagen
import numpy as np

#Casteo a tensor
from tensorflow.keras.preprocessing.image import img_to_array

#Tensorflow
import tensorflow as tf
from tensorflow.keras import layers, models

#Capas modelo
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import Activation, Dense, Flatten, Conv2D, MaxPooling2D, concatenate, GlobalAveragePooling2D, Lambda, Input, BatchNormalization, SeparableConv2D

#Separación de datos
from sklearn import model_selection

#Obtener resultados
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error

#Early stopping
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

#Keras tuner

import keras_tuner as kt
#from keras_tuner.tuners import GridSearch
#from keras_tuner.tuners import Hyperband

"""##Módulo de montaje##"""

'''
Módulo de montaje
'''

'''
def montaje_drive():
  drive.mount('/content/gdrive', force_remount=True)
  os.chdir('/content/gdrive/MyDrive/TFG/')
'''
"""##Variables globales##"""


if len(sys.argv) < 5:
    print("Debes proporcionar al menos cuatro argumentos desde la terminal.")
    sys.exit(1)

nuevo_ancho = int(sys.argv[1])
nuevo_alto = nuevo_ancho
canales = int(sys.argv[2])
numero_modelo = int(sys.argv[3])
epocas = int(sys.argv[4])
optimizador = "adam"
perdida = "mean_squared_error"

print(f"Ancho: {nuevo_ancho}")
print(f"Alto: {nuevo_alto}")
print(f"Canales: {canales}")
print(f"Modelo: {numero_modelo}")
print(f"Epocas: {epocas}")

"""##Módulo de carga de datos##"""

'''
Módulo de carga de datos
'''
#Csv con el grado de parecido
#Esta opción permite leer un csv y devolver lo obtenido
def leer_csv(nombre_csv, separador, decimal):
  csv = read_csv(nombre_csv, sep=separador, decimal=decimal)
  return csv


#Obtener las imágenes a partir de la ruta de drive
def obtener_imagenes(ruta_imagenes):
  lista_imagenes = []
  imagenes_corruptas = []
  with tarfile.open(ruta_imagenes, 'r:gz') as tar:  #Extracción de las imágenes del archivo comprimido
      for miembro in tar.getmembers():
          if miembro.isfile():
              #De cada miembro, se extrae su información
              contenido_miembro = tar.extractfile(miembro).read()

              try: #Este try comprueba si se puede leer la imagen, pues hay imágenes corruptas
                  imagen = Image.open(io.BytesIO(contenido_miembro))
                  lista_imagenes.append((miembro.name, imagen)) #Tomo de cada miembro el nombre para asociarlo a la imagen
              except Exception as e:
                  corrupta = miembro.name.replace('imagenes/', '')
                  corrupta = corrupta.replace('.png', '')
                  imagenes_corruptas.append(corrupta)


  columnas = ['Nombre', 'Imagen']
  dataset_imagenes = pd.DataFrame(lista_imagenes, columns=columnas)
  dataset_imagenes['Nombre'] = dataset_imagenes['Nombre'].str.replace('imagenes/', '')
  dataset_imagenes['Nombre'] = dataset_imagenes['Nombre'].str.replace('.png', '')
  return dataset_imagenes, imagenes_corruptas


#Esta función permite eliminar las imágenes que se hayan detectado como corruptas
def eliminar_corruptas(asociaciones, imagenes_corruptas):
  eliminar=[]
  #Como hay varias imágenes corruptas, las elimino de las asociaciones
  for indice, fila in asociaciones.iterrows():
        if fila['query'] in imagenes_corruptas or fila['ligando'] in imagenes_corruptas:
            eliminar.append(indice)
  asociaciones = asociaciones.drop(eliminar)
  return asociaciones

"""##Módulo de visionado de imágenes##"""

'''
Módulo de visionado de imágenes

def mostrar_imagen(dataset_imagenes, indice_imagen):
  nombre_imagen = dataset_imagenes.loc[indice_imagen, 'Nombre']
  imagen = dataset_imagenes.loc[indice_imagen, 'Imagen']

  # Muestra la imagen
  plt.imshow(imagen)
  plt.title(f"Nombre: {nombre_imagen}")
  plt.axis('off')  # Con esta opción no se muestran los recuadros
  plt.show()

def mostrar_numpy_array(imagen):
  plt.imshow(imagen, cmap='gray')
  plt.show()
'''

"""##Módulo de preprocesamiento##

###Tamaño imágenes###
"""

'''
Reduzco los píxeles de la imagen para facilitar el trabajo del modelo
'''


def formatear_tamano_imagenes(imagenes, nuevo_ancho, nuevo_alto):
  for i in range(len(imagenes)):
    tamaño_despues_preprocesado = (nuevo_ancho, nuevo_alto)
    imagenes[i] = imagenes[i].resize(tamaño_despues_preprocesado, Image.LANCZOS)
  return imagenes

"""###Conversión a escala de grises###"""

def conversion_escala_grises(dataset_imagenes):
  for i in range(len(dataset_imagenes['Imagen'])):
    dataset_imagenes['Imagen'][i] = ImageOps.grayscale(dataset_imagenes['Imagen'][i])

  canales = dataset_imagenes['Imagen'][0].split()
  return canales

"""###Asociación de imágenes###"""

'''
Este módulo busca tomar los datasets "dataset_imagenes" y "asociaciones" y crear un nuevo dataset con las dos imágenes ya preprocesadas y su puntuación final
'''

def crear_asociaciones(asociaciones, dataset_imagenes):
  datos_entrenar=[]
  for fila in asociaciones.itertuples(index=False):
    #Tomo los tres valores de cada fila de asociaciones
    primer_id = fila.query
    segundo_id = fila.ligando
    puntuacion = fila.score
    #Busco los ids en el dataset_imagenes
    try:
      primera_imagen = dataset_imagenes[dataset_imagenes['Nombre'] == primer_id].values[0]
    except Exception as e:
      print(f"La imagen {primer_id} no se ha encontrado")

    try:
      segunda_imagen = dataset_imagenes[dataset_imagenes['Nombre'] == segundo_id].values[0]
    except Exception as e:
      print(f"La imagen {segundo_id} no se ha encontrado")

    if primera_imagen is not None and segunda_imagen is not None:
      datos_entrenar.append((primera_imagen[1], segunda_imagen[1], puntuacion))

  dataset_entrenar = pd.DataFrame(datos_entrenar, columns=['primera_imagen', 'segunda_imagen', 'puntuacion'])
  return dataset_entrenar

"""###Casteo a tensor###"""

def pasar_a_array(imagen):
    imagen_array = img_to_array(imagen)
    return imagen_array

def castear_a_tensor(dataset_entrenar):
  dataset_entrenar['primera_imagen'] = dataset_entrenar['primera_imagen'].apply(pasar_a_array)
  dataset_entrenar['segunda_imagen'] = dataset_entrenar['segunda_imagen'].apply(pasar_a_array)
  return dataset_entrenar

"""

###Normalización###"""

def normalizacion(dataset_entrenar):
  for i in range(len(dataset_entrenar['primera_imagen'])):
    dataset_entrenar['primera_imagen'][i] = dataset_entrenar['primera_imagen'][i] / 255
    dataset_entrenar['segunda_imagen'][i] = dataset_entrenar['segunda_imagen'][i] / 255
  return dataset_entrenar

"""###Intercambio blanco por negro###"""

def intercambio_blanco_negro(dataset_entrenar):
  for i in range(len(dataset_entrenar['primera_imagen'])):
    dataset_entrenar['primera_imagen'][i] = 1 - dataset_entrenar['primera_imagen'][i]
    dataset_entrenar['segunda_imagen'][i] = 1 - dataset_entrenar['segunda_imagen'][i]
  return dataset_entrenar

"""###Eliminar dimensión canal###"""

def eliminar_dimension_canal(dataset_entrenar):
  for i in range(len(dataset_entrenar['primera_imagen'])):
    dataset_entrenar['primera_imagen'][i] = dataset_entrenar['primera_imagen'][i][:, :, 0]
    dataset_entrenar['segunda_imagen'][i] = dataset_entrenar['segunda_imagen'][i][:, :, 0]
  return dataset_entrenar

"""###Recortar datos###"""

def eliminar_filas(dataset_entrenar):
  filas = int(len(dataset_entrenar)/2)
  dataset_entrenar = dataset_entrenar.drop(dataset_entrenar.index[:filas])
  return dataset_entrenar

"""###Eliminar ceros###"""

def eliminar_ceros(dataset_entrenar):
  dataset_entrenar = dataset_entrenar[dataset_entrenar['puntuacion'] != 0]
  return dataset_entrenar

"""##Módulo separación datos##"""

def separar_datos(dataset_entrenar, tamano_validacion, tamano_test, semilla):
  caracteristicas1 = np.stack(dataset_entrenar['primera_imagen'].to_numpy())
  caracteristicas2 = np.stack(dataset_entrenar['segunda_imagen'].to_numpy())
  objetivo = dataset_entrenar['puntuacion'].astype('float32')

  #caracteristicas1_train, caracteristicas1_val, caracteristicas2_train, caracteristicas2_val, etiquetas_train, etiquetas_val = model_selection.train_test_split(
  #    caracteristicas1, caracteristicas2, objetivo, test_size=tamano_test, random_state=semilla)

  caracteristicas1_train, caracteristicas1_temp, caracteristicas2_train, caracteristicas2_temp, etiquetas_train, etiquetas_temp = model_selection.train_test_split(
      caracteristicas1, caracteristicas2, objetivo, test_size=tamano_validacion, random_state=semilla)

  caracteristicas1_val, caracteristicas1_test, caracteristicas2_val, caracteristicas2_test, etiquetas_val, etiquetas_test = model_selection.train_test_split(
      caracteristicas1_temp, caracteristicas2_temp, etiquetas_temp, test_size=tamano_test, random_state=semilla)

  return caracteristicas1_train, caracteristicas1_val, caracteristicas1_test, caracteristicas2_train, caracteristicas2_val, caracteristicas2_test, etiquetas_train, etiquetas_val, etiquetas_test

"""##Módulo creación modelos##

Este modelo sirve como prueba inicial para ver la mejora del rendimiento de los distintos modelos.

###Modelo Básico###
"""

def crear_modelo_basico(hp):
  filtros = hp.Choice('filtros_basico_1', values=[128, 256, 512, 1024])
  filtros2 = hp.Choice('filtros_basico_2', values=[128, 256, 512])
  filtros3 = hp.Choice('filtros_basico_3', values=[128, 256, 512])
  filtros4 = hp.Choice('filtros_basico_4', values=[8, 16, 32, 64, 128, 256])
  #filtros5 = hp.Choice('filtros_basico_5', values=[8, 16, 32, 128])
  kernel = hp.Choice('kernels_basico', values=[2])
  neuronas = hp.Choice('neuronas_basico', values=[1024, 2048, 4096])
  #neuronas2 = hp.Choice('neuronas_basico_2', values=[32, 64, 128, 256])
  #neuronas3 = hp.Choice('neuronas_basico_3', values=[8, 16, 32, 64])
  #neuronas4 = hp.Choice('neuronas_basico_4', values=[4, 8, 16, 32])
 
  modelo1 = Sequential([
      Conv2D(filters=filtros, kernel_size=kernel, input_shape=(nuevo_ancho, nuevo_alto, canales), activation='relu'),
      Conv2D(filters=filtros2, kernel_size=kernel, activation='relu'),
      Conv2D(filters=filtros3, kernel_size=kernel, activation='relu'),
      Conv2D(filters=filtros4, kernel_size=kernel, activation='relu'),
      #Conv2D(filters=filtros5, kernel_size=kernel, activation='relu'),
      MaxPooling2D(pool_size=(2,2)),
      Flatten()
  ])

  modelo2 = Sequential([
      Conv2D(filters=filtros, kernel_size=kernel, input_shape=(nuevo_ancho, nuevo_alto, canales), activation='relu'),
      Conv2D(filters=filtros2, kernel_size=kernel, activation='relu'),
      Conv2D(filters=filtros3, kernel_size=kernel, activation='relu'),
      Conv2D(filters=filtros4, kernel_size=kernel, activation='relu'),
      #Conv2D(filters=filtros5, kernel_size=kernel, activation='relu'),
      MaxPooling2D(pool_size=(2,2)),
      Flatten()
  ])

  concatenacion = concatenate([modelo1.output, modelo2.output])


  capa_densa = Dense(neuronas, activation='relu')(concatenacion)
  #capa_densa = Dense(neuronas2, activation='relu')(capa_densa)
  #capa_densa = Dense(neuronas3, activation='relu')(capa_densa)
  #capa_densa = Dense(neuronas4, activation='relu')(capa_densa)
  capa_output = Dense(1, activation='linear')(capa_densa)


  modeloFinal = Model(inputs=[modelo1.input, modelo2.input], outputs=capa_output)

  modeloFinal.compile(optimizer=optimizador, loss=perdida)

  return modeloFinal


"""###Modelo Lambda###"""

def crear_modelo_medio(hp):
  filtros = hp.Choice('filtros_lambda', values=[16, 32, 64, 128, 256, 512, 1024])
  kernel = hp.Choice('kernels_lambda', values=[2, 4, 8, 16])
  neuronas = hp.Choice('neuronas_lambda', values=[32, 64, 128, 256, 512, 1024])

  input_layer1 = Input(shape=(nuevo_ancho, nuevo_alto, canales))
  input_layer2 = Input(shape=(nuevo_ancho, nuevo_alto, canales))

  modelo1 = Conv2D(filters=filtros, kernel_size=kernel, activation='relu', padding='same')(input_layer1)
  modelo1 = MaxPooling2D(pool_size=(2,2)) (modelo1)
  modelo1 = GlobalAveragePooling2D() (modelo1)

  modelo2 = Conv2D(filters=filtros, kernel_size=kernel, activation='relu', padding='same')(input_layer2)
  modelo2 = MaxPooling2D(pool_size=(2,2)) (modelo2)
  modelo2 = GlobalAveragePooling2D() (modelo2)

  #concatenacion = concatenate([modelo1.output, modelo2.output])

  resta = Lambda(lambda x: abs(x[0] - x[1]))([modelo1, modelo2])

  capa_densa = Dense(neuronas, activation='relu')(resta)
  capa_output = Dense(1, activation='linear')(capa_densa)


  modeloFinal = Model(inputs=[input_layer1, input_layer2], outputs=capa_output)

  modeloFinal.compile(optimizer=optimizador, loss=perdida)

  return modeloFinal

"""###Modelo XCeption###"""

'''
def entry_flow(inputs) :

    x = Conv2D(32, 3, strides = 2, padding='same')(inputs)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv2D(64,3,padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    previous_block_activation = x

    for size in [128, 256, 728] :

        x = Activation('relu')(x)
        x = SeparableConv2D(size, 3, padding='same')(x)
        x = BatchNormalization()(x)

        x = Activation('relu')(x)
        x = SeparableConv2D(size, 3, padding='same')(x)
        x = BatchNormalization()(x)

        x = MaxPooling2D(3, strides=2, padding='same')(x)

        residual = Conv2D(size, 1, strides=2, padding='same')(previous_block_activation)

        x = tf.keras.layers.Add()([x, residual])
        previous_block_activation = x

    return x

def middle_flow(x, num_blocks=8) :

    previous_block_activation = x

    for _ in range(num_blocks) :

        x = Activation('relu')(x)
        x = SeparableConv2D(728, 3, padding='same')(x)
        x = BatchNormalization()(x)

        x = Activation('relu')(x)
        x = SeparableConv2D(728, 3, padding='same')(x)
        x = BatchNormalization()(x)

        x = Activation('relu')(x)
        x = SeparableConv2D(728, 3, padding='same')(x)
        x = BatchNormalization()(x)

        x = tf.keras.layers.Add()([x, previous_block_activation])
        previous_block_activation = x

    return x
def exit_flow(x) :

    previous_block_activation = x

    x = Activation('relu')(x)
    x = SeparableConv2D(728, 3, padding='same')(x)
    x = BatchNormalization()(x)

    x = Activation('relu')(x)
    x = SeparableConv2D(1024, 3, padding='same')(x)
    x = BatchNormalization()(x)

    x = MaxPooling2D(3, strides=2, padding='same')(x)

    residual = Conv2D(1024, 1, strides=2, padding='same')(previous_block_activation)
    x = tf.keras.layers.Add()([x, residual])

    x = Activation('relu')(x)
    x = SeparableConv2D(728, 3, padding='same')(x)
    x = BatchNormalization()(x)

    x = Activation('relu')(x)
    x = SeparableConv2D(1024, 3, padding='same')(x)
    x = BatchNormalization()(x)

    x = GlobalAveragePooling2D()(x)
    #x = Dense(1, activation='linear')(x)

    return x

def crear_modelo_xception(nuevo_alto, nuevo_ancho, canales):
  primer_input = Input(shape=(nuevo_alto, nuevo_ancho, canales))
  primer_output = exit_flow(middle_flow(entry_flow(primer_input)))

  segundo_input = Input(shape=(nuevo_alto, nuevo_ancho, canales))
  segundo_output = exit_flow(middle_flow(entry_flow(segundo_input)))

  concatenacion = concatenate([primer_output, segundo_output])

  capa_output = Dense(1, activation='linear')(concatenacion)

  modeloFinal = Model(inputs=[primer_input, segundo_input], outputs=capa_output)

  return modeloFinal
'''

"""##Módulo de compilación##"""

'''
Compilación del modelo
'''
def compilar_modelo(modelo, optimizador, perdida):
  modelo.compile(optimizer = optimizador,loss = perdida)

"""
##Módulo de entrenamiento##"""

def entrenar_modelo(modelo, caracteristicas1_train, caracteristicas2_train, etiquetas_train, epocas, batch_size, caracteristicas1_val, caracteristicas2_val, etiquetas_val):
  historial = modelo.fit([caracteristicas1_train, caracteristicas2_train], etiquetas_train , validation_data=(
      [caracteristicas1_val, caracteristicas2_val], etiquetas_val), epochs=epocas, verbose=0, batch_size=batch_size,
             callbacks=[
    EarlyStopping(monitor='val_loss', mode='min', min_delta=0, patience=40, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.5, patience=20, min_delta=0, min_lr=0.00001)]
             )
  return historial

"""##Módulo de predicción##"""

def predecir_modelo(modelo, caracteristicas1_test, caracteristicas2_test):
  predicciones = modelo.predict([caracteristicas1_test, caracteristicas2_test])
  return predicciones

"""##Módulo de resultados##"""

'''
Cálculo de las métricas de R cuadrado, MSE, RMSE, MAE, MAPE
'''

def obtener_resultados(etiquetas_test, predicciones):
  mse = mean_squared_error(etiquetas_test, predicciones)
  print(f'MSE: {mse}')

  mae = mean_absolute_error(etiquetas_test, predicciones)
  print(f'MAE: {mae}')

  suma_errors = 0
  epsilon = 0.0001  # Un valor muy pequeño
  for true_val, pred_val in zip(etiquetas_test, predicciones):
      true_val = max(true_val, epsilon)
      suma_errors += abs((true_val - pred_val) / true_val)
  mape2 = (suma_errors / len(etiquetas_test)) * 100
  print(f'MAPE: {mape2}')

  rmse = mean_squared_error(etiquetas_test, predicciones, squared=False)
  print("RMSE:", rmse)

  r2 = r2_score(etiquetas_test, predicciones)
  print("R2:", r2)

"""##Módulo de ejecución##

###Ejecución carga datos###
"""

def cargar_datos(csv, url_imagenes):
  #Ejecutar módulo de carga de datos, con la opción de csv
  asociaciones = leer_csv(csv, ',', '.')

  #Ejecutar módulo de carga de datos, con la opción de leer de Drive
  dataset_imagenes, imagenes_corruptas = obtener_imagenes(url_imagenes)

  #Ejecutar módulo de carga de datos, con la opción de eliminar las imágenes corruptas
  asociaciones = eliminar_corruptas(asociaciones, imagenes_corruptas)
  return asociaciones, dataset_imagenes

"""###Ejecución preprocesamiento###"""

def preprocesar(dataset_imagenes, nuevo_ancho, nuevo_alto, opciones):
  '''
  Cambiar tamaño imágenes
  '''
  if opciones[0] == True:
    dataset_imagenes['Imagen'] = formatear_tamano_imagenes(dataset_imagenes['Imagen'], nuevo_ancho, nuevo_alto)

  #Ejemplo del resultado
  print("Tamaño: ")
  print(dataset_imagenes['Imagen'][0].size)


  '''
  Cambiar a escala de grises
  '''
  if opciones[1] == True:
    canales = conversion_escala_grises(dataset_imagenes)
    print("Cambiado a gris")


  '''
  Asociar imágenes a su puntuación
  '''
  dataset_entrenar = crear_asociaciones(asociaciones, dataset_imagenes)


  '''
  Castear a tensor
  '''
  dataset_entrenar = castear_a_tensor(dataset_entrenar)

  '''
  Normalización
  '''
  if opciones[2] == True:
    dataset_entrenar = normalizacion(dataset_entrenar)
    print("Normalizado")

  '''
  Paso de blanco a negro
  '''
  if opciones[3] == True:
    dataset_entrenar = intercambio_blanco_negro(dataset_entrenar)
    print("De blanco a negro")

  '''
  Eliminar dimensión canal
  '''
  if opciones[4] == True:
    dataset_entrenar = eliminar_dimension_canal(dataset_entrenar)
    print("Eliminada dimension canal")
  '''
  Eliminar filas
  '''
  if opciones[5] == True:
    dataset_entrenar = eliminar_filas(dataset_entrenar)
    print("Filas eliminadas")

  '''
  Eliminar ceros
  '''
  if opciones[6] == True:
    dataset_entrenar = eliminar_ceros(dataset_entrenar)
    print("Ceros eliminados")


  return dataset_entrenar

"""###Ejecución de modelos###

####Ejecución modelo básico####
"""

def ejecutar_basico(epocas, batch_size, canales, datos, nuevo_ancho, nuevo_alto):
  print("Modelo básico")
  #modelo = crear_modelo_basico(kt.HyperParameters())
  #modeloBasico = crear_modelo_basico(nuevo_ancho, nuevo_alto, canales)
  tunerBasico = kt.BayesianOptimization(
    crear_modelo_basico,
    objective='val_loss',
    alpha=0.0000001,
    beta=20,
    overwrite=True
  )

  tunerBasico.search([datos['caracteristicas1_train'], datos['caracteristicas2_train']], datos['etiquetas_train'],
             epochs=50,
             batch_size=batch_size,
             verbose=0,
             validation_data=([datos['caracteristicas1_val'], datos['caracteristicas2_val']], datos['etiquetas_val']),
             callbacks=[
    EarlyStopping(monitor='val_loss', mode='min', min_delta=0, patience=40, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.5, patience=10, min_delta=0, min_lr=0.000001)])


  #compilar_modelo(modeloBasico, "adam", "mean_squared_error")
  #predicciones_basico = predecir_modelo(modeloBasico, datos['caracteristicas1_test'], datos['caracteristicas2_test'])

  mejores_hiperparametros = tunerBasico.get_best_hyperparameters(num_trials=1)[0]
  mejor_modelo_Basico = tunerBasico.hypermodel.build(mejores_hiperparametros)
  entrenar_modelo(mejor_modelo_Basico, datos['caracteristicas1_train'], datos['caracteristicas2_train'], datos['etiquetas_train'], epocas, batch_size, datos['caracteristicas1_val'], datos['caracteristicas2_val'], datos['etiquetas_val'])
  predicciones_basico = predecir_modelo(mejor_modelo_Basico, datos['caracteristicas1_test'], datos['caracteristicas2_test'])
  print("Hiperparámetros: ")
  print(mejores_hiperparametros.values)
  print("Resultados: ")
  obtener_resultados(datos['etiquetas_test'], predicciones_basico)
  return predicciones_basico

"""####Ejecución modelo lambda####"""

def ejecutar_lambda(epocas, batch_size, canales, datos, nuevo_ancho, nuevo_alto):

  print("Modelo lambda")
  tunerLambda = kt.BayesianOptimization(
    crear_modelo_medio,
    objective='val_loss',
    alpha=0.0000001,
    beta=20,
    overwrite=True
  )

  tunerLambda.search([datos['caracteristicas1_train'], datos['caracteristicas2_train']], datos['etiquetas_train'],
             epochs=50,
             batch_size=1000,
             verbose=0,
             validation_data=([datos['caracteristicas1_val'], datos['caracteristicas2_val']], datos['etiquetas_val']),
             callbacks=[
  EarlyStopping(monitor='val_loss', mode='min', min_delta=0, patience=20, restore_best_weights=True, verbose=1),
  ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.5, patience=10, min_delta=0, min_lr=0.000001)])

  mejores_hiperparametros = tunerLambda.get_best_hyperparameters(num_trials=1)[0]
  mejor_modelo_lambda = tunerLambda.hypermodel.build(mejores_hiperparametros)
  historial = entrenar_modelo(mejor_modelo_lambda, datos['caracteristicas1_train'], datos['caracteristicas2_train'], datos['etiquetas_train'], epocas, batch_size, datos['caracteristicas1_val'], datos['caracteristicas2_val'], datos['etiquetas_val'])
  predicciones_lambda = predecir_modelo(mejor_modelo_lambda, datos['caracteristicas1_test'], datos['caracteristicas2_test'])
  print("Hiperparámetros: ")
  print(mejores_hiperparametros.values)
  print("Resultados: ")
  obtener_resultados(datos['etiquetas_test'], predicciones_lambda)

  train_losses = historial.history['loss']
  val_losses = historial.history['val_loss']

  plt.plot(train_losses, label='Training Loss')
  plt.plot(val_losses, label='Validation Loss')
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.title('Learning Curve')
  plt.legend()

  plt.savefig('learning_curve.png')

  return predicciones_lambda

  #modeloLambda = crear_modelo_medio(nuevo_ancho, nuevo_alto, canales)
  #compilar_modelo(modeloLambda, "adam", "mean_squared_error")
  #entrenar_modelo(modeloLambda, datos['caracteristicas1_train'], datos['caracteristicas2_train'], datos['etiquetas_train'], epocas, batch_size, datos['caracteristicas1_val'], datos['caracteristicas2_val'], datos['etiquetas_val'])
  #predicciones_lambda = predecir_modelo(modeloLambda, datos['caracteristicas1_test'], datos['caracteristicas2_test'])

  #print()
  #print("Resultados: ")
  #obtener_resultados(datos['etiquetas_test'], predicciones_lambda)
  #return predicciones_lambda

"""#Selección de la ejecución#

##Selección carga y preprocesamiento##
"""

#montaje_drive()

csv = 'dataset-v2.csv'
#url_imagenes = '/content/gdrive/MyDrive/TFG/imagenes.tgz'
url_imagenes = 'imagenes.tgz'

opciones = [None] * 7
#Tamano
opciones[0] = True
#Gris
if canales == 1:
  opciones[1] = True
else:
  opciones[1] = False
#Normalizar
opciones[2] = True
#Blanco-negro
opciones[3] = True
#Eliminar canal
opciones[4] = False
#Eliminar filas
opciones[5] = False
#Eliminar 0
opciones[6] = False

with tf.device('/GPU:0'):
  asociaciones, dataset_imagenes = cargar_datos(csv, url_imagenes)
  dataset_entrenar = preprocesar(dataset_imagenes, nuevo_ancho, nuevo_alto, opciones)
pass

"""##Selección separación datos##"""

tamano_entrenamiento = 0.4
tamano_test = 0.5
semilla = 1

caracteristicas1_train, caracteristicas1_val, caracteristicas1_test, caracteristicas2_train, caracteristicas2_val, caracteristicas2_test, etiquetas_train, etiquetas_val, etiquetas_test = separar_datos(
    dataset_entrenar, tamano_entrenamiento, tamano_test, semilla)

datos = {
    'caracteristicas1_train': caracteristicas1_train,
    'caracteristicas1_val': caracteristicas1_val,
    'caracteristicas1_test': caracteristicas1_test,
    'caracteristicas2_train': caracteristicas2_train,
    'caracteristicas2_val': caracteristicas2_val,
    'caracteristicas2_test': caracteristicas2_test,
    'etiquetas_train': etiquetas_train,
    'etiquetas_val': etiquetas_val,
    'etiquetas_test': etiquetas_test
}
print(dataset_entrenar.shape)

"""##Selección modelo##"""

#epocas=10
batch_size=200
#canales=1

with tf.device('/GPU:0'):
  for i in range(1):
    if numero_modelo == 0:
      prediccionesBasico = ejecutar_basico(epocas, batch_size, canales, datos, nuevo_ancho, nuevo_alto)
    elif numero_modelo == 1:
    elif numero_modelo == 2:
      prediccionesLambda = ejecutar_lambda(epocas, batch_size, canales, datos, nuevo_ancho, nuevo_alto)
pass
